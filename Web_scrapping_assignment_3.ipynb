{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721e2df4",
   "metadata": {},
   "source": [
    "## 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c4dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb614cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_product(product):\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = f\"{base_url}/s?k={product.replace(' ', '+')}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            product_list = soup.find_all(\"span\", attrs={\"class\": \"a-size-medium a-color-base a-text-normal\"})\n",
    "            \n",
    "            if product_list:\n",
    "                print(f\"Products related to '{product}' on Amazon India:\")\n",
    "                for product in product_list:\n",
    "                    print(product.text)\n",
    "            else:\n",
    "                print(f\"No products found related to '{product}'\")\n",
    "        else:\n",
    "            print(\"Failed to fetch data from Amazon. Please try again later.\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon India: \")\n",
    "    search_amazon_product(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab1042",
   "metadata": {},
   "source": [
    "## 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29deea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product):\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = f\"{base_url}/s?k={product.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    product_data = []\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(search_url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                products = soup.find_all(\"div\", attrs={\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "                if not products:\n",
    "                    break\n",
    "\n",
    "                for prod in products:\n",
    "                    product_info = {}\n",
    "\n",
    "                    # Extracting details\n",
    "                    product_title = prod.find(\"span\", attrs={\"class\": \"a-size-medium a-color-base a-text-normal\"})\n",
    "                    if product_title:\n",
    "                        product_info['Name of the Product'] = product_title.text.strip()\n",
    "                    else:\n",
    "                        product_info['Name of the Product'] = '-'\n",
    "\n",
    "                    product_brand = prod.find(\"span\", attrs={\"class\": \"a-size-base-plus a-color-base\"})\n",
    "                    if product_brand:\n",
    "                        product_info['Brand Name'] = product_brand.text.strip()\n",
    "                    else:\n",
    "                        product_info['Brand Name'] = '-'\n",
    "\n",
    "                    product_price = prod.find(\"span\", attrs={\"class\": \"a-price-whole\"})\n",
    "                    if product_price:\n",
    "                        product_info['Price'] = '₹' + product_price.text.strip()\n",
    "                    else:\n",
    "                        product_info['Price'] = '-'\n",
    "\n",
    "                    product_url = prod.find(\"a\", attrs={\"class\": \"a-link-normal a-text-normal\"})\n",
    "                    if product_url:\n",
    "                        product_info['Product URL'] = base_url + product_url['href']\n",
    "                    else:\n",
    "                        product_info['Product URL'] = '-'\n",
    "\n",
    "                    # Fetching more details from the product page\n",
    "                    if product_info['Product URL'] != '-':\n",
    "                        product_page = requests.get(product_info['Product URL'], headers=headers)\n",
    "                        if product_page.status_code == 200:\n",
    "                            page_soup = BeautifulSoup(product_page.content, \"html.parser\")\n",
    "                            product_details = page_soup.find_all(\"div\", attrs={\"class\": \"a-section a-spacing-mini\"})\n",
    "\n",
    "                            for detail in product_details:\n",
    "                                label = detail.find(\"span\", attrs={\"class\": \"a-size-base-plus a-color-base\"})\n",
    "                                value = detail.find(\"span\", attrs={\"class\": \"a-size-base\"})\n",
    "                                if label and value:\n",
    "                                    product_info[label.text.strip()] = value.text.strip()\n",
    "                                elif label:\n",
    "                                    product_info[label.text.strip()] = '-'\n",
    "\n",
    "                    product_data.append(product_info)\n",
    "\n",
    "                # Moving to the next page if available\n",
    "                next_page = soup.find(\"li\", {\"class\": \"a-last\"})\n",
    "                if not next_page or page_number >= 3:\n",
    "                    break\n",
    "                page_number += 1\n",
    "                search_url = base_url + next_page.find('a')['href']\n",
    "            else:\n",
    "                print(\"Failed to fetch data from Amazon. Please try again later.\")\n",
    "                break\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    return product_data\n",
    "\n",
    "def create_csv(product_data):\n",
    "    df = pd.DataFrame(product_data)\n",
    "    df.to_csv('amazon_products.csv', index=False)\n",
    "    print(\"Data saved to 'amazon_products.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon India: \")\n",
    "    scraped_data = scrape_product_details(user_input)\n",
    "    create_csv(scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e3c49",
   "metadata": {},
   "source": [
    "## 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Function to create a directory if it doesn't exist\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def scrape_images(keywords, num_images=10):\n",
    "    # Set up Chrome webdriver (provide path to your chromedriver executable)\n",
    "    driver_path = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "    # Maximize the browser window\n",
    "    driver.maximize_window()\n",
    "\n",
    "    for keyword in keywords:\n",
    "        create_directory(keyword)\n",
    "        url = f\"https://www.google.com/search?q={keyword}&tbm=isch\"\n",
    "\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "        # Scroll to load more images dynamically\n",
    "        for _ in range(5):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Find and store image URLs\n",
    "        img_urls = set()\n",
    "        images = driver.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "        count = 0\n",
    "        for image in images:\n",
    "            if count >= num_images:\n",
    "                break\n",
    "            try:\n",
    "                image.click()\n",
    "                time.sleep(2)\n",
    "                actual_images = driver.find_elements_by_css_selector('img.n3VNCb')\n",
    "                for actual_image in actual_images:\n",
    "                    if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                        img_urls.add(actual_image.get_attribute('src'))\n",
    "                        count += 1\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        # Save images\n",
    "        for i, img_url in enumerate(img_urls):\n",
    "            try:\n",
    "                response = requests.get(img_url)\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                img.save(f\"{keyword}/image_{i+1}.jpg\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image: {e}\")\n",
    "\n",
    "    # Quit the driver\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    scrape_images(keywords, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e8a44",
   "metadata": {},
   "source": [
    "## 4.Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphones(product):\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = f\"{base_url}/search?q={product.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    product_data = []\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            products = soup.find_all(\"div\", {\"class\": \"_1AtVbE\"})\n",
    "\n",
    "            for product in products:\n",
    "                product_info = {}\n",
    "\n",
    "                # Extracting details\n",
    "                product_title = product.find(\"div\", {\"class\": \"_4rR01T\"})\n",
    "                if product_title:\n",
    "                    product_info['Smartphone name'] = product_title.text.strip()\n",
    "                else:\n",
    "                    product_info['Smartphone name'] = '-'\n",
    "\n",
    "                product_brand = product.find(\"div\", {\"class\": \"_2WkVRV\"})\n",
    "                if product_brand:\n",
    "                    product_info['Brand Name'] = product_brand.text.split()[0]\n",
    "                else:\n",
    "                    product_info['Brand Name'] = '-'\n",
    "\n",
    "                product_url = product.find(\"a\", {\"class\": \"_1fQZEK\"})\n",
    "                if product_url:\n",
    "                    product_info['Product URL'] = base_url + product_url['href']\n",
    "                else:\n",
    "                    product_info['Product URL'] = '-'\n",
    "\n",
    "                product_price = product.find(\"div\", {\"class\": \"_30jeq3 _1_WHN1\"})\n",
    "                if product_price:\n",
    "                    product_info['Price'] = product_price.text.strip()\n",
    "                else:\n",
    "                    product_info['Price'] = '-'\n",
    "\n",
    "                # Fetching more details from the product page\n",
    "                if product_info['Product URL'] != '-':\n",
    "                    product_page = requests.get(product_info['Product URL'], headers=headers)\n",
    "                    if product_page.status_code == 200:\n",
    "                        page_soup = BeautifulSoup(product_page.content, \"html.parser\")\n",
    "                        specs = page_soup.find_all(\"li\", {\"class\": \"_21lJbe\"})\n",
    "\n",
    "                        for spec in specs:\n",
    "                            key = spec.find(\"li\", {\"class\": \"_21lJbe\"})\n",
    "                            value = spec.find(\"li\", {\"class\": \"_21lJbe\"})\n",
    "                            if key and value:\n",
    "                                product_info[key.text.strip()] = value.text.strip()\n",
    "                            elif key:\n",
    "                                product_info[key.text.strip()] = '-'\n",
    "\n",
    "                product_data.append(product_info)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return product_data\n",
    "\n",
    "def create_csv(product_data):\n",
    "    df = pd.DataFrame(product_data)\n",
    "    df.to_csv('flipkart_smartphones.csv', index=False)\n",
    "    print(\"Data saved to 'flipkart_smartphones.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the smartphone to search on Flipkart: \")\n",
    "    scraped_data = scrape_smartphones(user_input)\n",
    "    create_csv(scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20eb80",
   "metadata": {},
   "source": [
    "## 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a330a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def get_coordinates(city):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    try:\n",
    "        driver.get(\"https://www.google.com/maps\")\n",
    "        search_box = driver.find_element_by_css_selector(\"input.tactile-searchbox-input\")\n",
    "        search_box.send_keys(city)\n",
    "        search_box.submit()\n",
    "\n",
    "        # Wait for the map to load\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # Get the current URL after searching for the city\n",
    "        current_url = driver.current_url\n",
    "\n",
    "        # Extract latitude and longitude from the URL\n",
    "        if \"/@\" in current_url:\n",
    "            coordinates = current_url.split(\"/@\")[1].split(\",\")[0:2]\n",
    "            latitude = coordinates[0]\n",
    "            longitude = coordinates[1]\n",
    "            print(f\"Geospatial Coordinates for {city}: Latitude - {latitude}, Longitude - {longitude}\")\n",
    "        else:\n",
    "            print(\"Coordinates not found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the city name: \")\n",
    "    get_coordinates(city_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d8003",
   "metadata": {},
   "source": [
    "## 6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    laptop_data = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            laptops = soup.find_all('div', class_='TopNumbeHeading active-class')\n",
    "\n",
    "            for laptop in laptops:\n",
    "                laptop_info = {}\n",
    "\n",
    "                laptop_name = laptop.find('div', class_='heading')\n",
    "                if laptop_name:\n",
    "                    laptop_info['Laptop Name'] = laptop_name.text.strip()\n",
    "                else:\n",
    "                    laptop_info['Laptop Name'] = '-'\n",
    "\n",
    "                details = laptop.find_next('div', class_='product-detail')\n",
    "                if details:\n",
    "                    features = details.find_all('div', class_='value')\n",
    "                    specs = details.find_all('div', class_='Specs-Wrap')\n",
    "\n",
    "                    for feature, spec in zip(features, specs):\n",
    "                        laptop_info[feature.text.strip()] = spec.text.strip()\n",
    "\n",
    "                laptop_data.append(laptop_info)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return laptop_data\n",
    "\n",
    "def create_csv(laptop_data):\n",
    "    df = pd.DataFrame(laptop_data)\n",
    "    df.to_csv('gaming_laptops_digit.csv', index=False)\n",
    "    print(\"Data saved to 'gaming_laptops_digit.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_data = scrape_gaming_laptops()\n",
    "    create_csv(scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b05749",
   "metadata": {},
   "source": [
    "## 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    billionaires_data = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            billionaires = soup.find_all('div', class_='person')\n",
    "\n",
    "            for billionaire in billionaires:\n",
    "                billionaire_info = {}\n",
    "\n",
    "                rank = billionaire.find('div', class_='rank')\n",
    "                if rank:\n",
    "                    billionaire_info['Rank'] = rank.text.strip()\n",
    "                else:\n",
    "                    billionaire_info['Rank'] = '-'\n",
    "\n",
    "                name = billionaire.find('div', class_='personName')\n",
    "                if name:\n",
    "                    billionaire_info['Name'] = name.text.strip()\n",
    "                else:\n",
    "                    billionaire_info['Name'] = '-'\n",
    "\n",
    "                net_worth = billionaire.find('div', class_='netWorth')\n",
    "                if net_worth:\n",
    "                    billionaire_info['Net worth'] = net_worth.text.strip()\n",
    "                else:\n",
    "                    billionaire_info['Net worth'] = '-'\n",
    "\n",
    "                age = billionaire.find('div', class_='age')\n",
    "                if age:\n",
    "                    billionaire_info['Age'] = age.text.strip()\n",
    "                else:\n",
    "                    billionaire_info['Age'] = '-'\n",
    "\n",
    "                citizenship = billionaire.find('div', class_='countryOfCitizenship')\n",
    "                if citizenship:\n",
    "                    billionaire_info['Citizenship'] = citizenship.text.strip()\n",
    "                else:\n",
    "                    billionaire_info['Citizenship'] = '-'\n",
    "\n",
    "                source = billionaire.find('div', class_='source-column')\n",
    "                if source:\n",
    "                    billionaire_info['Source'] = source.text.strip()\n",
    "                else:\n",
    "                    billionaire_info['Source'] = '-'\n",
    "\n",
    "                industry = billionaire.find('div', class_='category')\n",
    "                if industry:\n",
    "                    billionaire_info['Industry'] = industry.text.strip()\n",
    "                else:\n",
    "                    billionaire_info['Industry'] = '-'\n",
    "\n",
    "                billionaires_data.append(billionaire_info)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return billionaires_data\n",
    "\n",
    "def create_csv(billionaires_data):\n",
    "    df = pd.DataFrame(billionaires_data)\n",
    "    df.to_csv('forbes_billionaires.csv', index=False)\n",
    "    print(\"Data saved to 'forbes_billionaires.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_data = scrape_forbes_billionaires()\n",
    "    create_csv(scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abe047",
   "metadata": {},
   "source": [
    "## 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!google-api-python-client\n",
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0aaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# YouTube API key\n",
    "api_key = serviceusage.googleapis.com\n",
    "\n",
    "# Create a YouTube service object\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Specify the video ID of the YouTube video\n",
    "video_id = 'xTVfhGleb5Q?si=PkfNIqXrGD7w4nrd'\n",
    "\n",
    "# Request comments for the specified video\n",
    "comments = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id,\n",
    "    maxResults=500  # Number of comments to retrieve\n",
    ").execute()\n",
    "\n",
    "# Extract comment details\n",
    "comment_data = []\n",
    "for comment in comments['items']:\n",
    "    snippet = comment['snippet']['topLevelComment']['snippet']\n",
    "    comment_text = snippet['textDisplay']\n",
    "    comment_upvotes = snippet['likeCount']\n",
    "    comment_time = snippet['publishedAt']\n",
    "\n",
    "    comment_info = {\n",
    "        'Comment': comment_text,\n",
    "        'Upvotes': comment_upvotes,\n",
    "        'Time Posted': comment_time\n",
    "    }\n",
    "    comment_data.append(comment_info)\n",
    "\n",
    "# Display or save comment data as needed\n",
    "print(comment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce884b0",
   "metadata": {},
   "source": [
    "## 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfaa688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = 'https://www.hostelworld.com/hostels/London'\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    hostel_data = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            hostels = soup.find_all('div', class_='fabresult')\n",
    "\n",
    "            for hostel in hostels:\n",
    "                hostel_info = {}\n",
    "\n",
    "                hostel_name = hostel.find('h2', class_='title')\n",
    "                if hostel_name:\n",
    "                    hostel_info['Hostel Name'] = hostel_name.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Hostel Name'] = '-'\n",
    "\n",
    "                distance = hostel.find('span', class_='distanceFrom')\n",
    "                if distance:\n",
    "                    hostel_info['Distance from City Centre'] = distance.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Distance from City Centre'] = '-'\n",
    "\n",
    "                ratings = hostel.find('div', class_='score orange big')\n",
    "                if ratings:\n",
    "                    hostel_info['Ratings'] = ratings.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Ratings'] = '-'\n",
    "\n",
    "                total_reviews = hostel.find('div', class_='reviews')\n",
    "                if total_reviews:\n",
    "                    hostel_info['Total Reviews'] = total_reviews.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Total Reviews'] = '-'\n",
    "\n",
    "                overall_reviews = hostel.find('div', class_='keyword')\n",
    "                if overall_reviews:\n",
    "                    hostel_info['Overall Reviews'] = overall_reviews.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Overall Reviews'] = '-'\n",
    "\n",
    "                price_privates = hostel.find('span', {'data-room-type': 'Private'})\n",
    "                if price_privates:\n",
    "                    hostel_info['Privates from Price'] = price_privates.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Privates from Price'] = '-'\n",
    "\n",
    "                price_dorms = hostel.find('span', {'data-room-type': 'Dorm'})\n",
    "                if price_dorms:\n",
    "                    hostel_info['Dorms from Price'] = price_dorms.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Dorms from Price'] = '-'\n",
    "\n",
    "                facilities = hostel.find('div', class_='facilities-label')\n",
    "                if facilities:\n",
    "                    hostel_info['Facilities'] = facilities.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Facilities'] = '-'\n",
    "\n",
    "                description = hostel.find('div', class_='rating-factors propCardRating clearfix')\n",
    "                if description:\n",
    "                    hostel_info['Property Description'] = description.text.strip()\n",
    "                else:\n",
    "                    hostel_info['Property Description'] = '-'\n",
    "\n",
    "                hostel_data.append(hostel_info)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return hostel_data\n",
    "\n",
    "def create_csv(hostel_data):\n",
    "    df = pd.DataFrame(hostel_data)\n",
    "    df.to_csv('hostels_in_london.csv', index=False)\n",
    "    print(\"Data saved to 'hostels_in_london.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_data = scrape_hostels_in_london()\n",
    "    create_csv(scraped_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
