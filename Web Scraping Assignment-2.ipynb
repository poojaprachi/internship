{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c72b4ea",
   "metadata": {},
   "source": [
    "### 1 : Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ee84759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (4.15.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3960208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By \n",
    "import time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "babdbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6863e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the naukri page on automated chrome browser\n",
    "\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76209106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering designation and location as required\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME , \"suggestor-input \")\n",
    "designation.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "468cdac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH , '/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input')\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6cdd365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME , \"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[] \n",
    "company_name=[] \n",
    "job_location=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping 10 job title from the given page\n",
    "    \n",
    "title_tags=driver.find_elements(By.XPATH,'//div[@class=\"cust-job-tuple layout-wrapper lay-2 sjw__tuple \"]/div/a')\n",
    "for i in title_tags[:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#scrapping location from the given page\n",
    "    \n",
    "location_tags=driver.find_elements(By.XPATH ,'//span[@class=\"ni-job-tuple-icon ni-job-tuple-icon-srp-location loc\"]')\n",
    "for i in location_tags[:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "    \n",
    "#scrapping company name from the given page\n",
    "    \n",
    "company_tags=driver.find_elements(By.XPATH ,'//span[@class=\" comp-dtls-wrap\"]/a[1]')\n",
    "for i in company_tags[:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "#scrapping experience from the given page\n",
    "    \n",
    "experience_tags=driver.find_elements(By.XPATH ,'//span[@class=\"expwdth\"]')\n",
    "for i in experience_tags[:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5ea4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(company_name), len(job_location), len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be9fbfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>K12 Techno Services</td>\n",
       "      <td>Bangalore Rural, Karnataka, Bangarapet, Karnataka</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>TEKsystems</td>\n",
       "      <td>Bengaluru(BTM Layout)</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Mobile Premier League (MPL)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>mPokket</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>RMS Technologies</td>\n",
       "      <td>Bangalore/ Bengaluru, Karnataka</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Medico Recruits</td>\n",
       "      <td>Bengaluru, Karnataka(6th block Koramangala)</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Simplilearn</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Systechcorp Inc</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Elfonze Technologies</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Title                      Company  \\\n",
       "0  Data Analyst          K12 Techno Services   \n",
       "1  Data Analyst                   TEKsystems   \n",
       "2  Data Analyst  Mobile Premier League (MPL)   \n",
       "3  Data Analyst                          ANZ   \n",
       "4  Data Analyst                      mPokket   \n",
       "5  Data Analyst             RMS Technologies   \n",
       "6  Data Analyst              Medico Recruits   \n",
       "7  Data Analyst                  Simplilearn   \n",
       "8  Data Analyst              Systechcorp Inc   \n",
       "9  Data Analyst         Elfonze Technologies   \n",
       "\n",
       "                                            location Experience  \n",
       "0  Bangalore Rural, Karnataka, Bangarapet, Karnataka    2-5 Yrs  \n",
       "1                              Bengaluru(BTM Layout)    3-6 Yrs  \n",
       "2                                Bangalore/Bengaluru    1-5 Yrs  \n",
       "3                                Bangalore/Bengaluru    3-7 Yrs  \n",
       "4                       Bangalore/Bengaluru, Kolkata    0-0 Yrs  \n",
       "5                    Bangalore/ Bengaluru, Karnataka    0-2 Yrs  \n",
       "6        Bengaluru, Karnataka(6th block Koramangala)    2-7 Yrs  \n",
       "7                                          Bengaluru    2-5 Yrs  \n",
       "8                                Bangalore/Bengaluru    1-3 Yrs  \n",
       "9                                Bangalore/Bengaluru   5-10 Yrs  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':job_title, 'Company':company_name, 'location':job_location, 'Experience':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4e6c4",
   "metadata": {},
   "source": [
    "### Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed7f2996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (4.15.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62f51847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By \n",
    "import time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b05bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "594f322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the naukri page on automated chrome browser\n",
    "\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f90948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering designation and location as required\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME , \"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8afd159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH , '/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input')\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "07451df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME , \"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d74eaf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[] \n",
    "company_name=[] \n",
    "job_location=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f2d54945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping 10 job title from the given page\n",
    "    \n",
    "title_tags=driver.find_elements(By.XPATH,'//div[@class=\"cust-job-tuple layout-wrapper lay-2 sjw__tuple \"]/div/a')\n",
    "for i in title_tags[:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#scrapping location from the given page\n",
    "    \n",
    "location_tags=driver.find_elements(By.XPATH ,'//span[@class=\"ni-job-tuple-icon ni-job-tuple-icon-srp-location loc\"]')\n",
    "for i in location_tags[:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "    \n",
    "#scrapping company name from the given page\n",
    "    \n",
    "company_tags=driver.find_elements(By.XPATH ,'//span[@class=\" comp-dtls-wrap\"]/a[1]')\n",
    "for i in company_tags[:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "#scrapping experience from the given page\n",
    "    \n",
    "experience_tags=driver.find_elements(By.XPATH ,'//span[@class=\"expwdth\"]')\n",
    "for i in experience_tags[:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd2ad733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(company_name), len(job_location), len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1be8b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experienced Data Scientist</td>\n",
       "      <td>Volvo Financial Services</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Knowledge Graph</td>\n",
       "      <td>Capco</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>8-13 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist HTHD</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist/ Senior Data Scientist</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Gurgaon/Gur...</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist III</td>\n",
       "      <td>Conduent</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - FinOps</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ericsson</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Assistant Manager/Manager</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Title                   Company  \\\n",
       "0                              Data Scientist                      Ford   \n",
       "1                  Experienced Data Scientist  Volvo Financial Services   \n",
       "2            Data Scientist - Knowledge Graph                     Capco   \n",
       "3                         Data Scientist HTHD                      Ford   \n",
       "4       Data Scientist/ Senior Data Scientist         Fractal Analytics   \n",
       "5                       Senior Data Scientist         Fractal Analytics   \n",
       "6                          Data Scientist III                  Conduent   \n",
       "7                     Data Scientist - FinOps                       IBM   \n",
       "8                              Data Scientist                  Ericsson   \n",
       "9  Data Scientist - Assistant Manager/Manager                   Genpact   \n",
       "\n",
       "                                            location Experience  \n",
       "0  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...    2-6 Yrs  \n",
       "1  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...    3-8 Yrs  \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...   8-13 Yrs  \n",
       "3  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...    2-6 Yrs  \n",
       "4  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...    4-9 Yrs  \n",
       "5  Bangalore/Bengaluru, Mumbai, Pune, Gurgaon/Gur...   6-10 Yrs  \n",
       "6                                Bangalore/Bengaluru    4-9 Yrs  \n",
       "7                  Bangalore/Bengaluru, Kochi/Cochin    4-8 Yrs  \n",
       "8                                Bangalore/Bengaluru    0-3 Yrs  \n",
       "9                                          Bengaluru    4-7 Yrs  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':job_title, 'Company':company_name, 'location':job_location, 'Experience':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200aee8",
   "metadata": {},
   "source": [
    "### Q3: In this question you have to scrape data using the filters available on the webpage\n",
    "\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "\n",
    "1. first get the web page https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scrapeddata.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26baf56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (4.15.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739da199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your chromedriver executable\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Set up Chrome options and the service\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "service = webdriver.chrome.service.Service(driver)\n",
    "webdriver.chrome.service.ServiceExecutable(driver).install()\n",
    "\n",
    "# Start the WebDriver with the service and options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to Shine.com\n",
    "driver.get('https://www.shine.com/')\n",
    "\n",
    "# Find the search input and enter \"Data Scientist\"\n",
    "search_input = driver.find_element(By.ID, 'q')\n",
    "search_input.send_keys('Data Scientist')\n",
    "\n",
    "# Click the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[@class='btn btn-lg btn-block btn-blue']\")\n",
    "search_button.click()\n",
    "\n",
    "# Apply location filter for Delhi/NCR\n",
    "location_filter = driver.find_element(By.XPATH, \"//span[contains(text(), 'Delhi/NCR')]\")\n",
    "location_filter.click()\n",
    "\n",
    "# Apply salary filter for 3-6 lakhs\n",
    "salary_filter = driver.find_element(By.XPATH, \"//span[contains(text(), '3 - 6 Lakhs')]\")\n",
    "salary_filter.click()\n",
    "\n",
    "# Wait for the job listings to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'search_listing')))\n",
    "\n",
    "# Scrape data for the first 10 job results\n",
    "job_data = []\n",
    "results = driver.find_elements(By.CLASS_NAME, 'search_listing')[:10]\n",
    "for result in results:\n",
    "    job_title = result.find_element(By.CLASS_NAME, 'job_title').text.strip()\n",
    "    company_name = result.find_element(By.CLASS_NAME, 'company_name').text.strip()\n",
    "    job_location = result.find_element(By.CLASS_NAME, 'job_location').text.strip()\n",
    "    experience_required = result.find_element(By.XPATH, \".//li[contains(text(), 'Experience')]/span\").text.strip()\n",
    "\n",
    "    job_info = {\n",
    "        'Job Title': job_title,\n",
    "        'Company Name': company_name,\n",
    "        'Job Location': job_location,\n",
    "        'Experience Required': experience_required\n",
    "    }\n",
    "    job_data.append(job_info)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59138e00",
   "metadata": {},
   "source": [
    "### Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "6. Brand\n",
    "7. ProductDescription\n",
    "8. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8744b233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (4.15.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "801d5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7dd0b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f983dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (22.0)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.0 webdriver-manager-4.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'C:\\Users\\pujap\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Start the WebDriver\n",
    "from selenium import webdriver\n",
    "\n",
    "chrome_driver_path = r'C:\\Users\\pujap\\AppData\\Roaming\\Python\\Python310\\Scripts\\chromedriver.exe'\n",
    "\n",
    "# Initialize Chrome WebDriver with the specified path\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Navigate to Flipkart sunglasses page\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "# Close the login popup if present\n",
    "try:\n",
    "    driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Search for sunglasses\n",
    "search_bar = driver.find_element_by_xpath(\"//input[@title='Search for products, brands and more']\")\n",
    "search_bar.send_keys('sunglasses\\n')  # Entering '\\n' for 'Enter' key\n",
    "\n",
    "# Scrape data for the first 100 sunglasses listings\n",
    "sunglasses_data = []\n",
    "count = 0\n",
    "while count < 100:\n",
    "    product_cards = driver.find_elements_by_xpath(\"//div[@class='_2B099V']\")\n",
    "    for card in product_cards:\n",
    "        try:\n",
    "            brand = card.find_element_by_class_name('_2WkVRV').text.strip()\n",
    "            description = card.find_element_by_class_name('_2mylT6').text.strip()\n",
    "            price = card.find_element_by_class_name('_30jeq3').text.strip()\n",
    "\n",
    "            sunglasses_info = {\n",
    "                'Brand': brand,\n",
    "                'Product Description': description,\n",
    "                'Price': price\n",
    "            }\n",
    "            sunglasses_data.append(sunglasses_info)\n",
    "            count += 1\n",
    "            if count == 100:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    # Scroll down to load more products\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(sunglasses_data)\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0234d65",
   "metadata": {},
   "source": [
    "### Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKART\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a42ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Navigate to the Flipkart iPhone 11 reviews page\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART')\n",
    "\n",
    "# Scrape data for the first 100 reviews\n",
    "reviews_data = []\n",
    "count = 0\n",
    "\n",
    "while count < 100:\n",
    "    reviews = driver.find_elements_by_xpath(\"//div[@class='_27M-vq']\")\n",
    "    for review in reviews:\n",
    "        try:\n",
    "            rating = review.find_element_by_xpath(\".//div[contains(@class, '_3LWZlK')]\").text\n",
    "            review_summary = review.find_element_by_xpath(\".//p[@class='_2-N8zT']\").text\n",
    "            full_review = review.find_element_by_xpath(\".//div[@class='t-ZTKy']\").text\n",
    "\n",
    "            review_info = {\n",
    "                'Rating': rating,\n",
    "                'Review Summary': review_summary,\n",
    "                'Full Review': full_review\n",
    "            }\n",
    "            reviews_data.append(review_info)\n",
    "            count += 1\n",
    "            if count == 100:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "    # Scroll down to load more reviews\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    time.sleep(2)  # Adding a short delay to let the page load\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(reviews_data)\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441f28d",
   "metadata": {},
   "source": [
    "### Q6: Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Navigate to Flipkart and search for sneakers\n",
    "driver.get('https://www.flipkart.com/')\n",
    "search_bar = driver.find_element(By.XPATH, \"//input[@title='Search for products, brands and more']\")\n",
    "search_bar.send_keys('sneakers\\n')  # Entering '\\n' for 'Enter' key\n",
    "\n",
    "# Scrape data for the first 100 sneakers listings\n",
    "sneakers_data = []\n",
    "count = 0\n",
    "\n",
    "while count < 100:\n",
    "    sneakers = driver.find_elements(By.XPATH, \"//div[@class='_2B099V']\")\n",
    "    for sneaker in sneakers:\n",
    "        try:\n",
    "            brand = sneaker.find_element(By.CLASS_NAME, '_2WkVRV').text.strip()\n",
    "            description = sneaker.find_element(By.CLASS_NAME, '_2mylT6').text.strip()\n",
    "            price = sneaker.find_element(By.CLASS_NAME, '_30jeq3').text.strip()\n",
    "\n",
    "            sneaker_info = {\n",
    "                'Brand': brand,\n",
    "                'Product Description': description,\n",
    "                'Price': price\n",
    "            }\n",
    "            sneakers_data.append(sneaker_info)\n",
    "            count += 1\n",
    "            if count == 100:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    # Scroll down to load more sneakers\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(sneakers_data)\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bc0f2",
   "metadata": {},
   "source": [
    "### Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Navigate to Amazon India\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "# Find the search input and enter \"Laptop\"\n",
    "search_input = driver.find_element(By.ID, 'twotabsearchtextbox')\n",
    "search_input.send_keys('Laptop')\n",
    "\n",
    "# Click the search icon\n",
    "search_button = driver.find_element(By.XPATH, \"//input[@value='Go']\")\n",
    "search_button.click()\n",
    "\n",
    "# Apply CPU Type filter for \"Intel Core i7\"\n",
    "cpu_filter = driver.find_element(By.XPATH, \"//li[@aria-label='Intel Core i7']/span/a\")\n",
    "cpu_filter.click()\n",
    "\n",
    "# Wait for the products to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@data-component-type='s-search-result']\")))\n",
    "\n",
    "# Scrape data for the first 10 laptops\n",
    "laptops_data = []\n",
    "laptops = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")[:10]\n",
    "\n",
    "for laptop in laptops:\n",
    "    try:\n",
    "        title = laptop.find_element(By.XPATH, \".//h2/a/span\").text.strip()\n",
    "        ratings = laptop.find_element(By.XPATH, \".//span[@class='a-icon-alt']\").get_attribute('innerHTML').split(' ')[0]\n",
    "        price = laptop.find_element(By.XPATH, \".//span[@class='a-price']/span[@class='a-offscreen']\").text.strip()\n",
    "\n",
    "        laptop_info = {\n",
    "            'Title': title,\n",
    "            'Ratings': ratings,\n",
    "            'Price': price\n",
    "        }\n",
    "        laptops_data.append(laptop_info)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(laptops_data)\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bcfea",
   "metadata": {},
   "source": [
    "### Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83add0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Navigate to AzQuotes website\n",
    "driver.get('https://www.azquotes.com/')\n",
    "\n",
    "# Click on Top Quotes link\n",
    "top_quotes_link = driver.find_element(By.XPATH, \"//a[contains(text(), 'Top Quotes')]\")\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Scrape data for Top 1000 Quotes\n",
    "quotes_data = []\n",
    "\n",
    "# Loop through each page to scrape quotes\n",
    "for page_num in range(1, 11):  # Assuming 1000 quotes are spread across 10 pages (100 quotes per page)\n",
    "    quotes_on_page = driver.find_elements(By.XPATH, \"//div[@class='wrap-blocks']/div\")\n",
    "    \n",
    "    for quote in quotes_on_page:\n",
    "        try:\n",
    "            quote_text = quote.find_element(By.CLASS_NAME, 'title').text.strip()\n",
    "            author = quote.find_element(By.CLASS_NAME, 'author').text.strip()\n",
    "            type_of_quote = quote.find_element(By.CLASS_NAME, 'kw-box').text.strip()\n",
    "            \n",
    "            quote_info = {\n",
    "                'Quote': quote_text,\n",
    "                'Author': author,\n",
    "                'Type Of Quote': type_of_quote\n",
    "            }\n",
    "            quotes_data.append(quote_info)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "    # Click on next page button\n",
    "    if page_num < 10:\n",
    "        next_page_button = driver.find_element(By.XPATH, \"//a[@class='btn -next']\")\n",
    "        next_page_button.click()\n",
    "\n",
    "# Print or process the scraped quotes data\n",
    "for idx, quote in enumerate(quotes_data, start=1):\n",
    "    print(f\"Quote {idx}: {quote['Quote']}\")\n",
    "    print(f\"Author: {quote['Author']}\")\n",
    "    print(f\"Type Of Quote: {quote['Type Of Quote']}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3974",
   "metadata": {},
   "source": [
    "### Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa237bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Navigate to Jagran Josh website\n",
    "driver.get('https://www.jagranjosh.com/')\n",
    "\n",
    "# Click on the GK option\n",
    "gk_option = driver.find_element(By.XPATH, \"//a[contains(text(),'GK')]\")\n",
    "gk_option.click()\n",
    "\n",
    "# Click on the List of all Prime Ministers of India\n",
    "prime_ministers_link = driver.find_element(By.LINK_TEXT, 'List of all Prime Ministers of India')\n",
    "prime_ministers_link.click()\n",
    "\n",
    "# Scrape data for former Prime Ministers\n",
    "prime_ministers_data = []\n",
    "\n",
    "# Find the table containing Prime Ministers' information\n",
    "table = driver.find_element(By.CLASS_NAME, 'article-tbl')\n",
    "\n",
    "# Get rows from the table\n",
    "rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "# Skip the header row (first row)\n",
    "for row in rows[1:]:\n",
    "    columns = row.find_elements(By.TAG_NAME, 'td')\n",
    "    \n",
    "    # Extracting data from columns\n",
    "    name = columns[0].text.strip()\n",
    "    born_dead = columns[1].text.strip()\n",
    "    term_of_office = columns[2].text.strip()\n",
    "    remarks = columns[3].text.strip()\n",
    "    \n",
    "    # Storing data in dictionary\n",
    "    prime_minister_info = {\n",
    "        'Name': name,\n",
    "        'Born-Dead': born_dead,\n",
    "        'Term of Office': term_of_office,\n",
    "        'Remarks': remarks\n",
    "    }\n",
    "    \n",
    "    # Appending data to the list\n",
    "    prime_ministers_data.append(prime_minister_info)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(prime_ministers_data)\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0732b73b",
   "metadata": {},
   "source": [
    "### Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f56ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Navigate to Motor1 website\n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "# Type in the search bar '50 most expensive cars'\n",
    "search_bar = driver.find_element(By.XPATH, \"//input[@class='search-field']\")\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.send_keys(Keys.ENTER)\n",
    "\n",
    "# Click on '50 most expensive cars in the world'\n",
    "expensive_cars_link = driver.find_element(By.XPATH, \"//h2[contains(text(),'50 most expensive cars in the world')]\")\n",
    "expensive_cars_link.click()\n",
    "\n",
    "# Scrape data for 50 most expensive cars\n",
    "cars_data = []\n",
    "\n",
    "# Find the list of cars\n",
    "cars_list = driver.find_elements(By.XPATH, \"//div[@class='article-body']//ol//li\")\n",
    "\n",
    "# Extracting data for each car\n",
    "for car in cars_list[:50]:  # Scraping data for the first 50 cars\n",
    "    try:\n",
    "        car_name = car.find_element(By.TAG_NAME, 'strong').text.strip()\n",
    "        car_price = car.find_element(By.TAG_NAME, 'span').text.strip()\n",
    "        \n",
    "        car_info = {\n",
    "            'Car Name': car_name,\n",
    "            'Price': car_price\n",
    "        }\n",
    "        cars_data.append(car_info)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(cars_data)\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a5567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
