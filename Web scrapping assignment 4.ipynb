{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f61dab1",
   "metadata": {},
   "source": [
    "### 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: \n",
    "A)Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f67c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\pujap\\appdata\\roaming\\python\\python310\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "    ranks = []\n",
    "    names = []\n",
    "    artists = []\n",
    "    upload_dates = []\n",
    "    views = []\n",
    "\n",
    "    for row in table.find_all(\"tr\")[1:]:  \n",
    "        columns = row.find_all(\"td\")\n",
    "        try:\n",
    "            rank = columns[0].text.strip()\n",
    "            name = columns[1].text.strip()\n",
    "            artist = columns[2].text.strip()\n",
    "            upload_date = columns[3].text.strip()\n",
    "            view = columns[4].text.strip()\n",
    "\n",
    "            # Append extracted data to lists\n",
    "            ranks.append(rank)\n",
    "            names.append(name)\n",
    "            artists.append(artist)\n",
    "            upload_dates.append(upload_date)\n",
    "            views.append(view)\n",
    "        except IndexError as e:\n",
    "            print(f\"Error: {e} - Skipping row due to missing data\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Display the extracted data\n",
    "    for i in range(len(ranks)):\n",
    "        print(f\"Rank: {ranks[i]}\")\n",
    "        print(f\"Name: {names[i]}\")\n",
    "        print(f\"Artist: {artists[i]}\")\n",
    "        print(f\"Upload Date: {upload_dates[i]}\")\n",
    "        print(f\"Views: {views[i]}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Request Exception: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c24596",
   "metadata": {},
   "source": [
    "### 2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv. Url = https://www.bcci.tv/. You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    nav_bar = soup.find(\"div\", {\"class\": \"navigation__menu\"})\n",
    "\n",
    "    if nav_bar:\n",
    "        nav_items = nav_bar.find_all(\"div\", {\"class\": \"navigation__drop-down\"})\n",
    "\n",
    "        for nav_item in nav_items:\n",
    "            if \"International\" in nav_item.text:\n",
    "                fixtures_link = nav_item.find(\"a\")['href']\n",
    "                break  \n",
    "\n",
    "        fixtures_url = f\"https://www.bcci.tv{fixtures_link}\"\n",
    "\n",
    "        fixtures_response = requests.get(fixtures_url)\n",
    "        fixtures_response.raise_for_status()  \n",
    "\n",
    "        fixtures_soup = BeautifulSoup(fixtures_response.text, \"html.parser\")\n",
    "\n",
    "        fixtures_section = fixtures_soup.find(\"div\", {\"class\": \"js-list\"})\n",
    "\n",
    "        if fixtures_section:\n",
    "            fixtures = fixtures_section.find_all(\"div\", {\"class\": \"fixture__info\"})\n",
    "            for fixture in fixtures:\n",
    "                try:\n",
    "                    series = fixture.find(\"p\", {\"class\": \"fixture__additional-info\"}).text.strip()\n",
    "                    place = fixture.find(\"p\", {\"class\": \"fixture__description\"}).text.strip()\n",
    "                    date_time = fixture.find(\"span\", {\"class\": \"fixture__datetime\"}).text.strip()\n",
    "\n",
    "                    date, time = date_time.split(\", \")\n",
    "                    date = date.strip()\n",
    "                    time = time.strip()\n",
    "\n",
    "                    print(\"Series:\", series)\n",
    "                    print(\"Place:\", place)\n",
    "                    print(\"Date:\", date)\n",
    "                    print(\"Time:\", time)\n",
    "                    print(\"-\" * 20)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "        else:\n",
    "            print(\"Fixtures section not found\")\n",
    "\n",
    "    else:\n",
    "        print(\"Navigation bar not found or 'International' link not present\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Request Exception: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64be2af",
   "metadata": {},
   "source": [
    "### 3. Scrape the details of State-wise GDP of India from statisticstime.com. Url = http://statisticstimes.com/. You have to find following details: \n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    econ_link = soup.find(\"a\", text=\"Economy\")\n",
    "    if econ_link:\n",
    "        econ_url = econ_link['href']\n",
    "        econ_url = url + econ_url  \n",
    "\n",
    "        econ_response = requests.get(econ_url)\n",
    "        econ_response.raise_for_status()  \n",
    "\n",
    "        econ_soup = BeautifulSoup(econ_response.text, \"html.parser\")\n",
    "\n",
    "        state_gdp_link = econ_soup.find(\"a\", text=\"GDP of Indian states\")\n",
    "        if state_gdp_link:\n",
    "            state_gdp_url = url + state_gdp_link['href']\n",
    "\n",
    "            state_gdp_response = requests.get(state_gdp_url)\n",
    "            state_gdp_response.raise_for_status()  \n",
    "\n",
    "            state_gdp_soup = BeautifulSoup(state_gdp_response.text, \"html.parser\")\n",
    "\n",
    "            table = state_gdp_soup.find(\"table\", {\"id\": \"table_id\"})\n",
    "\n",
    "            rows = table.find_all(\"tr\")\n",
    "            for row in rows[1:]:  \n",
    "                columns = row.find_all(\"td\")\n",
    "                try:\n",
    "                    rank = columns[0].text.strip()\n",
    "                    state = columns[1].text.strip()\n",
    "                    gsdp_1819 = columns[2].text.strip()\n",
    "                    gsdp_1920 = columns[3].text.strip()\n",
    "                    share_1819 = columns[4].text.strip()\n",
    "                    gdp_billion = columns[5].text.strip()\n",
    "\n",
    "                    print(\"Rank:\", rank)\n",
    "                    print(\"State:\", state)\n",
    "                    print(\"GSDP(18-19):\", gsdp_1819)\n",
    "                    print(\"GSDP(19-20):\", gsdp_1920)\n",
    "                    print(\"Share(18-19):\", share_1819)\n",
    "                    print(\"GDP($ billion):\", gdp_billion)\n",
    "                    print(\"-\" * 20)\n",
    "\n",
    "                except IndexError as e:\n",
    "                    print(f\"Error: {e} - Skipping row due to missing data\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "\n",
    "        else:\n",
    "            print(\"State-wise GDP link not found\")\n",
    "\n",
    "    else:\n",
    "        print(\"Economy link not found\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Request Exception: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c2811",
   "metadata": {},
   "source": [
    "### 4. Scrape the details of trending repositories on Github.com. Url = https://github.com/. You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used \n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcfdcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Setting Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.headless = False  \n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "webdriver_path = 'path_to_chromedriver.exe'  \n",
    "\n",
    "service = Service(webdriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "try:\n",
    "    driver.get(\"https://github.com/\")\n",
    "\n",
    "    explore_button = driver.find_element(By.XPATH, \"//summary[contains(text(), 'Explore')]\")\n",
    "    explore_button.click()\n",
    "\n",
    "    trending_option = driver.find_element(By.XPATH, \"//a[contains(text(), 'Trending')]\")\n",
    "    trending_option.click()\n",
    "\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    trending_repos = driver.find_elements(By.CSS_SELECTOR, \"article.Box-row\")\n",
    "\n",
    "    for repo in trending_repos:\n",
    "        try:\n",
    "            # Repository title\n",
    "            title = repo.find_element(By.CSS_SELECTOR, \"h1.h3\").text.strip()\n",
    "\n",
    "            # Repository description\n",
    "            description = repo.find_element(By.CSS_SELECTOR, \"p.f4\").text.strip()\n",
    "\n",
    "            # Contributors count\n",
    "            contributors_count = len(repo.find_elements(By.CSS_SELECTOR, \"a.avatar-user\"))\n",
    "\n",
    "            # Language used\n",
    "            language_elem = repo.find_element(By.CSS_SELECTOR, \"span[itemprop='programmingLanguage']\")\n",
    "            language = language_elem.text.strip() if language_elem else \"Not specified\"\n",
    "\n",
    "            print(\"Repository Title:\", title)\n",
    "            print(\"Description:\", description)\n",
    "            print(\"Contributors Count:\", contributors_count)\n",
    "            print(\"Language Used:\", language)\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Closing the browser window\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e33ce",
   "metadata": {},
   "source": [
    "### 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Set Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.headless = False  # Set True to run headless (without opening browser window)\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Provide the path to your Chrome WebDriver\n",
    "webdriver_path = 'path_to_chromedriver.exe'  # Change this to your WebDriver path\n",
    "\n",
    "# Create a WebDriver instance\n",
    "service = Service(webdriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Open Billboard homepage\n",
    "    driver.get(\"https://www.billboard.com/\")\n",
    "\n",
    "    # Click on the \"Charts\" dropdown menu\n",
    "    charts_button = driver.find_element(By.XPATH, \"//a[@aria-label='Charts']\")\n",
    "    charts_button.click()\n",
    "\n",
    "    # Click on the \"Hot 100\" link\n",
    "    hot_100_link = driver.find_element(By.XPATH, \"//a[@data-analytics-title='hot-100']\")\n",
    "    hot_100_link.click()\n",
    "\n",
    "    # Wait for the Hot 100 page to load\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Get details of top 100 songs\n",
    "    songs = driver.find_elements(By.XPATH, \"//li[contains(@class, 'chart-list__element')]\")\n",
    "\n",
    "    for song in songs:\n",
    "        try:\n",
    "            # Song name\n",
    "            song_name = song.find_element(By.CLASS_NAME, \"chart-element__information__song\").text\n",
    "\n",
    "            # Artist name\n",
    "            artist_name = song.find_element(By.CLASS_NAME, \"chart-element__information__artist\").text\n",
    "\n",
    "            # Last week rank\n",
    "            last_week_rank = song.find_element(By.CLASS_NAME, \"chart-element__meta.text--center.color--secondary.text--last\").text\n",
    "\n",
    "            # Peak rank\n",
    "            peak_rank = song.find_element(By.CLASS_NAME, \"chart-element__meta.text--center.color--secondary.text--peak\").text\n",
    "\n",
    "            # Weeks on board\n",
    "            weeks_on_board = song.find_element(By.CLASS_NAME, \"chart-element__meta.text--center.color--secondary.text--week\").text\n",
    "\n",
    "            print(\"Song Name:\", song_name)\n",
    "            print(\"Artist Name:\", artist_name)\n",
    "            print(\"Last Week Rank:\", last_week_rank)\n",
    "            print(\"Peak Rank:\", peak_rank)\n",
    "            print(\"Weeks on Board:\", weeks_on_board)\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser window\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441a19c",
   "metadata": {},
   "source": [
    "### 6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    " \n",
    "Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing the highest selling novels data\n",
    "    table = soup.find(\"table\", {\"class\": \"in-article sortable\"})\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    book_names = []\n",
    "    author_names = []\n",
    "    volumes_sold = []\n",
    "    publishers = []\n",
    "    genres = []\n",
    "\n",
    "    # Extract data from the table\n",
    "    rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        columns = row.find_all(\"td\")\n",
    "        try:\n",
    "            book_name = columns[1].text.strip()\n",
    "            author_name = columns[2].text.strip()\n",
    "            volume_sold = columns[3].text.strip()\n",
    "            publisher = columns[4].text.strip()\n",
    "            genre = columns[5].text.strip()\n",
    "\n",
    "            book_names.append(book_name)\n",
    "            author_names.append(author_name)\n",
    "            volumes_sold.append(volume_sold)\n",
    "            publishers.append(publisher)\n",
    "            genres.append(genre)\n",
    "\n",
    "        except IndexError as e:\n",
    "            print(f\"Error: {e} - Skipping row due to missing data\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Display the extracted data\n",
    "    for i in range(len(book_names)):\n",
    "        print(f\"Book Name: {book_names[i]}\")\n",
    "        print(f\"Author Name: {author_names[i]}\")\n",
    "        print(f\"Volumes Sold: {volumes_sold[i]}\")\n",
    "        print(f\"Publisher: {publishers[i]}\")\n",
    "        print(f\"Genre: {genres[i]}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Request Exception: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7f001",
   "metadata": {},
   "source": [
    "### 7. Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e73ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the container holding TV series details\n",
    "    series_container = soup.find(\"div\", class_=\"lister-list\")\n",
    "\n",
    "    # Find all TV series listed\n",
    "    series_list = series_container.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    names = []\n",
    "    year_spans = []\n",
    "    genres = []\n",
    "    run_times = []\n",
    "    ratings = []\n",
    "    votes = []\n",
    "\n",
    "    # Extract data for each TV series\n",
    "    for series in series_list:\n",
    "        try:\n",
    "            # Name\n",
    "            name = series.find(\"h3\").a.text.strip()\n",
    "            names.append(name)\n",
    "\n",
    "            # Year span\n",
    "            year_span = series.find(\"span\", class_=\"lister-item-year\").text.strip()\n",
    "            year_spans.append(year_span)\n",
    "\n",
    "            # Genre\n",
    "            genre = series.find(\"span\", class_=\"genre\").text.strip()\n",
    "            genres.append(genre)\n",
    "\n",
    "            # Run time\n",
    "            runtime = series.find(\"span\", class_=\"runtime\").text.strip()\n",
    "            run_times.append(runtime)\n",
    "\n",
    "            # Ratings\n",
    "            rating = float(series.find(\"span\", class_=\"ipl-rating-star__rating\").text.strip())\n",
    "            ratings.append(rating)\n",
    "\n",
    "            # Votes\n",
    "            vote = int(series.find(\"span\", {\"name\": \"nv\"}).text.replace(\",\", \"\"))\n",
    "            votes.append(vote)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Display the extracted data\n",
    "    for i in range(len(names)):\n",
    "        print(f\"Name: {names[i]}\")\n",
    "        print(f\"Year Span: {year_spans[i]}\")\n",
    "        print(f\"Genre: {genres[i]}\")\n",
    "        print(f\"Run Time: {run_times[i]}\")\n",
    "        print(f\"Ratings: {ratings[i]}\")\n",
    "        print(f\"Votes: {votes[i]}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Request Exception: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44d44b",
   "metadata": {},
   "source": [
    "### 8. Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute \n",
    "G) Year\n",
    "\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb45152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the UCI Machine Learning Repository homepage\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find and navigate to the Show All Dataset page\n",
    "    show_all_datasets_link = soup.find(\"a\", href=\"ml/datasets.php\")\n",
    "    if show_all_datasets_link:\n",
    "        show_all_datasets_url = url + show_all_datasets_link[\"href\"]\n",
    "\n",
    "        # Send a GET request to the Show All Dataset page\n",
    "        datasets_response = requests.get(show_all_datasets_url)\n",
    "        datasets_response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the Show All Dataset page\n",
    "        datasets_soup = BeautifulSoup(datasets_response.content, \"html.parser\")\n",
    "\n",
    "        # Find the table containing dataset details\n",
    "        table = datasets_soup.find(\"table\", {\"cellspacing\": \"3\"})\n",
    "\n",
    "        # Initialize lists to store data\n",
    "        dataset_names = []\n",
    "        data_types = []\n",
    "        tasks = []\n",
    "        attribute_types = []\n",
    "        instances = []\n",
    "        attributes = []\n",
    "        years = []\n",
    "\n",
    "        # Extract data from the table\n",
    "        rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "        for row in rows:\n",
    "            columns = row.find_all(\"td\")\n",
    "            try:\n",
    "                dataset_name = columns[0].text.strip()\n",
    "                dataset_names.append(dataset_name)\n",
    "\n",
    "                data_type = columns[1].text.strip()\n",
    "                data_types.append(data_type)\n",
    "\n",
    "                task = columns[2].text.strip()\n",
    "                tasks.append(task)\n",
    "\n",
    "                attribute_type = columns[3].text.strip()\n",
    "                attribute_types.append(attribute_type)\n",
    "\n",
    "                instance = columns[4].text.strip()\n",
    "                instances.append(instance)\n",
    "\n",
    "                attribute = columns[5].text.strip()\n",
    "                attributes.append(attribute)\n",
    "\n",
    "                year = columns[6].text.strip()\n",
    "                years.append(year)\n",
    "\n",
    "            except IndexError as e:\n",
    "                print(f\"Error: {e} - Skipping row due to missing data\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "        # Display the extracted data\n",
    "        for i in range(len(dataset_names)):\n",
    "            print(f\"Dataset Name: {dataset_names[i]}\")\n",
    "            print(f\"Data Type: {data_types[i]}\")\n",
    "            print(f\"Task: {tasks[i]}\")\n",
    "            print(f\"Attribute Type: {attribute_types[i]}\")\n",
    "            print(f\"Number of Instances: {instances[i]}\")\n",
    "            print(f\"Number of Attributes: {attributes[i]}\")\n",
    "            print(f\"Year: {years[i]}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    else:\n",
    "        print(\"Show All Dataset link not found\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Request Exception: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff108d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
